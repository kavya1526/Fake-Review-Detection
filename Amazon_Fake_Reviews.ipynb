{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IIZZuEZ4Y0cc","executionInfo":{"status":"ok","timestamp":1713760556358,"user_tz":420,"elapsed":21115,"user":{"displayName":"Ninad Parthiv Shah","userId":"14593699598119062697"}},"outputId":"f6fab86b-62d4-4b81-ed2b-4f1b09f16ea3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","from tqdm import tqdm\n","\n","file_path = '/content/drive/MyDrive/fake_reviews_dataset.csv'\n","df = pd.read_csv(file_path)\n","\n","# Preprocessing\n","# Here, we are using CountVectorizer to convert text into numerical features\n","vectorizer = CountVectorizer(max_features=5000)\n","X_text = vectorizer.fit_transform(df['text_']).toarray()\n","\n","# Label encoding for 'category' and 'label'\n","le_category = LabelEncoder()\n","df['category'] = le_category.fit_transform(df['category'])\n","\n","le_label = LabelEncoder()\n","df['label'] = le_label.fit_transform(df['label'])\n","\n","# Split the data into training and testing sets\n","X_train_text, X_test_text, y_train, y_test = train_test_split(X_text, df['label'].values, test_size=0.2, random_state=42)\n","X_train_category, X_test_category = train_test_split(df['category'].values, test_size=0.2, random_state=42)\n","X_train_rating, X_test_rating = train_test_split(df['rating'].values, test_size=0.2, random_state=42)\n","\n","# Define a custom dataset class\n","class FakeReviewDataset(Dataset):\n","    def __init__(self, text, category, rating, labels):\n","        self.text = torch.tensor(text, dtype=torch.float32)\n","        self.category = torch.tensor(category, dtype=torch.float32)\n","        self.rating = torch.tensor(rating, dtype=torch.float32)\n","        self.labels = torch.tensor(labels, dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'text': self.text[idx],\n","            'category': self.category[idx],\n","            'rating': self.rating[idx],\n","            'labels': self.labels[idx]\n","        }\n","\n","# Create DataLoader for training and testing\n","batch_size = 32\n","train_dataset = FakeReviewDataset(X_train_text, X_train_category, X_train_rating, y_train)\n","test_dataset = FakeReviewDataset(X_test_text, X_test_category, X_test_rating, y_test)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Define the neural network model\n","class FakeReviewModel(nn.Module):\n","    def __init__(self, input_dim):\n","        super(FakeReviewModel, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 2)\n","\n","    def forward(self, x_text, x_category, x_rating):\n","        x = torch.cat([x_text, x_category.unsqueeze(1), x_rating.unsqueeze(1)], dim=1)\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# Initialize the model, loss function, and optimizer\n","input_dim = X_text.shape[1] + 2  # Features from text, category, and rating\n","model = FakeReviewModel(input_dim)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training the model\n","num_epochs = 10\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n","        text = batch['text'].to(device)\n","        category = batch['category'].to(device)\n","        rating = batch['rating'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(text, category, rating)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    average_loss = total_loss / len(train_loader)\n","    print(f'Training Loss: {average_loss:.4f}')\n","\n","# Evaluate the model\n","model.eval()\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for batch in tqdm(test_loader, desc='Evaluating'):\n","        text = batch['text'].to(device)\n","        category = batch['category'].to(device)\n","        rating = batch['rating'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(text, category, rating)\n","        _, predicted = torch.max(outputs, 1)\n","\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = correct / total\n","print(f'Test Accuracy: {accuracy * 100:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcQ34Klbw4vd","executionInfo":{"status":"ok","timestamp":1713760602510,"user_tz":420,"elapsed":46158,"user":{"displayName":"Ninad Parthiv Shah","userId":"14593699598119062697"}},"outputId":"96d7f5a7-badb-455a-ff1e-3740f7bf3c49"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10: 100%|██████████| 1011/1011 [00:03<00:00, 277.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.2585\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10: 100%|██████████| 1011/1011 [00:02<00:00, 380.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.1513\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10: 100%|██████████| 1011/1011 [00:02<00:00, 378.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.0858\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10: 100%|██████████| 1011/1011 [00:02<00:00, 380.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.0410\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10: 100%|██████████| 1011/1011 [00:03<00:00, 291.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.0225\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10: 100%|██████████| 1011/1011 [00:02<00:00, 378.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.0147\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10: 100%|██████████| 1011/1011 [00:02<00:00, 373.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.0087\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10: 100%|██████████| 1011/1011 [00:02<00:00, 372.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.0065\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10: 100%|██████████| 1011/1011 [00:02<00:00, 344.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.0080\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10: 100%|██████████| 1011/1011 [00:03<00:00, 328.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 0.0053\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 253/253 [00:00<00:00, 616.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 91.42%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","from tqdm import tqdm\n","from sklearn.model_selection import ParameterGrid\n","\n","# Load the dataset\n","file_path = '/content/drive/MyDrive/fake_reviews_dataset.csv'\n","df = pd.read_csv(file_path)\n","\n","# Preprocessing\n","vectorizer = CountVectorizer(max_features=5000)\n","X_text = vectorizer.fit_transform(df['text_']).toarray()\n","\n","le_category = LabelEncoder()\n","df['category'] = le_category.fit_transform(df['category'])\n","\n","le_label = LabelEncoder()\n","df['label'] = le_label.fit_transform(df['label'])\n","\n","X_train_text, X_test_text, y_train, y_test = train_test_split(X_text, df['label'].values, test_size=0.2, random_state=42)\n","X_train_category, X_test_category = train_test_split(df['category'].values, test_size=0.2, random_state=42)\n","X_train_rating, X_test_rating = train_test_split(df['rating'].values, test_size=0.2, random_state=42)\n","\n","class FakeReviewDataset(Dataset):\n","    def __init__(self, text, category, rating, labels):\n","        self.text = torch.tensor(text, dtype=torch.float32)\n","        self.category = torch.tensor(category, dtype=torch.float32)\n","        self.rating = torch.tensor(rating, dtype=torch.float32)\n","        self.labels = torch.tensor(labels, dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'text': self.text[idx],\n","            'category': self.category[idx],\n","            'rating': self.rating[idx],\n","            'labels': self.labels[idx]\n","        }\n","\n","batch_size = 32\n","train_dataset = FakeReviewDataset(X_train_text, X_train_category, X_train_rating, y_train)\n","test_dataset = FakeReviewDataset(X_test_text, X_test_category, X_test_rating, y_test)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","class FakeReviewModel(nn.Module):\n","    def __init__(self, input_dim):\n","        super(FakeReviewModel, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 2)\n","\n","    def forward(self, x_text, x_category, x_rating):\n","        x = torch.cat([x_text, x_category.unsqueeze(1), x_rating.unsqueeze(1)], dim=1)\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        for i, batch in enumerate(train_loader):\n","            text = batch['text'].to(device)\n","            category = batch['category'].to(device)\n","            rating = batch['rating'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(text, category, rating)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        average_loss = total_loss / len(train_loader)\n","        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_loss:.4f}')\n","\n","def evaluate_model(model, test_loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(test_loader):\n","            text = batch['text'].to(device)\n","            category = batch['category'].to(device)\n","            rating = batch['rating'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(text, category, rating)\n","            _, predicted = torch.max(outputs, 1)\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    if total == 0:\n","        return None\n","    else:\n","        accuracy = correct / total\n","        print(f'Test Accuracy: {accuracy * 100:.2f}%')\n","        return accuracy\n","\n","# Hyperparameters\n","input_dim = X_text.shape[1] + 2\n","num_epochs = 10\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Grid search for the best weight decay parameter\n","# Hyperparameter tuning for number of epochs, optimizer, and criterion\n","param_grid = {\n","    'learning_rate': [0.001, 0.01, 0.1],\n","    'num_epochs': [10, 15],\n","}\n","\n","best_accuracy = 0\n","best_params = {}\n","\n","print('Hyperparameter tuning started...')\n","\n","for params in tqdm(list(ParameterGrid(param_grid)), desc='Hyperparameter tuning'):\n","    learning_rate = params['learning_rate']\n","    num_epochs = params['num_epochs']\n","    optimizer = optim.Adam\n","    criterion = nn.CrossEntropyLoss()\n","\n","    model = FakeReviewModel(input_dim).to(device)\n","    optimizer_instance = optimizer(model.parameters(), lr=learning_rate)\n","\n","    print(f'Training with parameters: {params}')\n","    train_model(model, train_loader, criterion, optimizer_instance, num_epochs, device)\n","    accuracy = evaluate_model(model, test_loader, device)\n","\n","    if accuracy is not None and accuracy > best_accuracy:\n","        best_accuracy = accuracy\n","        best_params = params\n","\n","print(\"Best hyperparameters:\", best_params)\n","print(\"Best accuracy:\", best_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CDMdvO8WOb90","executionInfo":{"status":"ok","timestamp":1713760822039,"user_tz":420,"elapsed":219533,"user":{"displayName":"Ninad Parthiv Shah","userId":"14593699598119062697"}},"outputId":"298787dd-57b7-425f-dcd9-18af211b0495"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Hyperparameter tuning started...\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter tuning:   0%|          | 0/6 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Training with parameters: {'learning_rate': 0.001, 'num_epochs': 10}\n","Epoch 1/10, Training Loss: 0.2606\n","Epoch 2/10, Training Loss: 0.1530\n","Epoch 3/10, Training Loss: 0.0895\n","Epoch 4/10, Training Loss: 0.0461\n","Epoch 5/10, Training Loss: 0.0240\n","Epoch 6/10, Training Loss: 0.0119\n","Epoch 7/10, Training Loss: 0.0117\n","Epoch 8/10, Training Loss: 0.0068\n","Epoch 9/10, Training Loss: 0.0035\n","Epoch 10/10, Training Loss: 0.0048\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter tuning:  17%|█▋        | 1/6 [00:29<02:25, 29.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 91.55%\n","Training with parameters: {'learning_rate': 0.001, 'num_epochs': 15}\n","Epoch 1/15, Training Loss: 0.2632\n","Epoch 2/15, Training Loss: 0.1554\n","Epoch 3/15, Training Loss: 0.0906\n","Epoch 4/15, Training Loss: 0.0481\n","Epoch 5/15, Training Loss: 0.0276\n","Epoch 6/15, Training Loss: 0.0119\n","Epoch 7/15, Training Loss: 0.0120\n","Epoch 8/15, Training Loss: 0.0085\n","Epoch 9/15, Training Loss: 0.0052\n","Epoch 10/15, Training Loss: 0.0038\n","Epoch 11/15, Training Loss: 0.0062\n","Epoch 12/15, Training Loss: 0.0050\n","Epoch 13/15, Training Loss: 0.0042\n","Epoch 14/15, Training Loss: 0.0025\n","Epoch 15/15, Training Loss: 0.0034\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter tuning:  33%|███▎      | 2/6 [01:15<02:37, 39.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 91.29%\n","Training with parameters: {'learning_rate': 0.01, 'num_epochs': 10}\n","Epoch 1/10, Training Loss: 0.2665\n","Epoch 2/10, Training Loss: 0.1552\n","Epoch 3/10, Training Loss: 0.1040\n","Epoch 4/10, Training Loss: 0.0680\n","Epoch 5/10, Training Loss: 0.0467\n","Epoch 6/10, Training Loss: 0.0370\n","Epoch 7/10, Training Loss: 0.0266\n","Epoch 8/10, Training Loss: 0.0199\n","Epoch 9/10, Training Loss: 0.0137\n","Epoch 10/10, Training Loss: 0.0190\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter tuning:  50%|█████     | 3/6 [01:44<01:43, 34.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 91.58%\n","Training with parameters: {'learning_rate': 0.01, 'num_epochs': 15}\n","Epoch 1/15, Training Loss: 0.2627\n","Epoch 2/15, Training Loss: 0.1504\n","Epoch 3/15, Training Loss: 0.0971\n","Epoch 4/15, Training Loss: 0.0562\n","Epoch 5/15, Training Loss: 0.0408\n","Epoch 6/15, Training Loss: 0.0291\n","Epoch 7/15, Training Loss: 0.0222\n","Epoch 8/15, Training Loss: 0.0196\n","Epoch 9/15, Training Loss: 0.0179\n","Epoch 10/15, Training Loss: 0.0169\n","Epoch 11/15, Training Loss: 0.0162\n","Epoch 12/15, Training Loss: 0.0159\n","Epoch 13/15, Training Loss: 0.0131\n","Epoch 14/15, Training Loss: 0.0140\n","Epoch 15/15, Training Loss: 0.0069\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter tuning:  67%|██████▋   | 4/6 [02:25<01:14, 37.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 91.03%\n","Training with parameters: {'learning_rate': 0.1, 'num_epochs': 10}\n","Epoch 1/10, Training Loss: 0.7558\n","Epoch 2/10, Training Loss: 0.7377\n","Epoch 3/10, Training Loss: 0.6965\n","Epoch 4/10, Training Loss: 0.6969\n","Epoch 5/10, Training Loss: 0.6973\n","Epoch 6/10, Training Loss: 0.6974\n","Epoch 7/10, Training Loss: 0.6964\n","Epoch 8/10, Training Loss: 0.6974\n","Epoch 9/10, Training Loss: 0.6968\n","Epoch 10/10, Training Loss: 0.6980\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter tuning:  83%|████████▎ | 5/6 [02:53<00:33, 33.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 49.66%\n","Training with parameters: {'learning_rate': 0.1, 'num_epochs': 15}\n","Epoch 1/15, Training Loss: 0.5572\n","Epoch 2/15, Training Loss: 0.3377\n","Epoch 3/15, Training Loss: 0.6989\n","Epoch 4/15, Training Loss: 0.6998\n","Epoch 5/15, Training Loss: 0.6962\n","Epoch 6/15, Training Loss: 0.6962\n","Epoch 7/15, Training Loss: 0.6966\n","Epoch 8/15, Training Loss: 0.6956\n","Epoch 9/15, Training Loss: 0.6960\n","Epoch 10/15, Training Loss: 0.6960\n","Epoch 11/15, Training Loss: 0.6963\n","Epoch 12/15, Training Loss: 0.6963\n","Epoch 13/15, Training Loss: 0.6966\n","Epoch 14/15, Training Loss: 0.6961\n","Epoch 15/15, Training Loss: 0.6959\n"]},{"output_type":"stream","name":"stderr","text":["Hyperparameter tuning: 100%|██████████| 6/6 [03:35<00:00, 35.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 49.66%\n","Best hyperparameters: {'learning_rate': 0.01, 'num_epochs': 10}\n","Best accuracy: 0.9157907753184122\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split, ParameterGrid  # Add ParameterGrid import\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from gensim.models import Word2Vec\n","\n","# Load the dataset\n","file_path = '/content/drive/MyDrive/fake_reviews_dataset.csv'\n","df = pd.read_csv(file_path)\n","\n","# TF-IDF Vectorization\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n","X_text_tfidf = tfidf_vectorizer.fit_transform(df['text_'])\n","\n","# Word2Vec Embeddings\n","sentences = [text.split() for text in df['text_']]\n","word2vec_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4, alpha=0.03, min_alpha=0.0007)\n","word2vec_model.train(sentences, total_examples=len(sentences), epochs=10)\n","\n","def text_to_word2vec(text):\n","    words = text.split()\n","    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n","    if len(word_vectors) == 0:\n","        return np.zeros(word2vec_model.vector_size)\n","    else:\n","        return np.mean(word_vectors, axis=0)\n","\n","X_text_word2vec = np.array([text_to_word2vec(text) for text in df['text_']])\n","X_text_combined = np.concatenate((X_text_tfidf.toarray(), X_text_word2vec), axis=1)\n","\n","le_category = LabelEncoder()\n","df['category'] = le_category.fit_transform(df['category'])\n","\n","le_label = LabelEncoder()\n","df['label'] = le_label.fit_transform(df['label'])\n","\n","X_train_text, X_test_text, y_train, y_test = train_test_split(X_text_combined, df['label'].values, test_size=0.2, random_state=42)\n","X_train_category, X_test_category = train_test_split(df['category'].values, test_size=0.2, random_state=42)\n","X_train_rating, X_test_rating = train_test_split(df['rating'].values, test_size=0.2, random_state=42)\n","\n","class FakeReviewDataset(Dataset):\n","    def __init__(self, text, category, rating, labels):\n","        self.text = torch.tensor(text, dtype=torch.float32)\n","        self.category = torch.tensor(category, dtype=torch.float32)\n","        self.rating = torch.tensor(rating, dtype=torch.float32)\n","        self.labels = torch.tensor(labels, dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'text': self.text[idx],\n","            'category': self.category[idx],\n","            'rating': self.rating[idx],\n","            'labels': self.labels[idx]\n","        }\n","\n","batch_size = 32\n","train_dataset = FakeReviewDataset(X_train_text, X_train_category, X_train_rating, y_train)\n","test_dataset = FakeReviewDataset(X_test_text, X_test_category, X_test_rating, y_test)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","class FakeReviewModel(nn.Module):\n","    def __init__(self, input_dim):\n","        super(FakeReviewModel, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 2)\n","\n","    def forward(self, x_text, x_category, x_rating):\n","        x = torch.cat([x_text, x_category.unsqueeze(1), x_rating.unsqueeze(1)], dim=1)\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        for i, batch in enumerate(train_loader):\n","            text = batch['text'].to(device)\n","            category = batch['category'].to(device)\n","            rating = batch['rating'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(text, category, rating)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        average_loss = total_loss / len(train_loader)\n","        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_loss:.4f}')\n","\n","def evaluate_model(model, test_loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(test_loader):\n","            text = batch['text'].to(device)\n","            category = batch['category'].to(device)\n","            rating = batch['rating'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(text, category, rating)\n","            _, predicted = torch.max(outputs, 1)\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    if total == 0:\n","        return None\n","    else:\n","        accuracy = correct / total\n","        print(f'Test Accuracy: {accuracy * 100:.2f}%')\n","        return accuracy\n","\n","# Hyperparameters and Grid Search\n","input_dim = X_text_combined.shape[1] + 2  # Adjusted for TF-IDF and word2vec\n","num_epochs = 10\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","param_grid = {\n","    'learning_rate': [0.001, 0.01, 0.1],\n","    'num_epochs': [10, 15],\n","}\n","\n","best_accuracy = 0\n","best_params = {}\n","\n","print('Hyperparameter tuning started...')\n","\n","for params in tqdm(list(ParameterGrid(param_grid)), desc='Hyperparameter tuning'):\n","    learning_rate = params['learning_rate']\n","    num_epochs = params['num_epochs']\n","    optimizer = optim.Adam\n","    criterion = nn.CrossEntropyLoss()\n","\n","    model = FakeReviewModel(input_dim).to(device)\n","    optimizer_instance = optimizer(model.parameters(), lr=learning_rate)\n","\n","    print(f'Training with parameters: {params}')\n","    train_model(model, train_loader, criterion, optimizer_instance, num_epochs, device)\n","    accuracy = evaluate_model(model, test_loader, device)\n","\n","    if accuracy is not None and accuracy > best_accuracy:\n","        best_accuracy = accuracy\n","        best_params = params\n","\n","print(\"Best hyperparameters:\", best_params)\n","print(\"Best accuracy:\", best_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f-6yfoLe_dPK","executionInfo":{"status":"ok","timestamp":1713575856533,"user_tz":420,"elapsed":275911,"user":{"displayName":"Ninad Parthiv Shah","userId":"14593699598119062697"}},"outputId":"a1f809ee-ad28-4e32-962d-9a23f00f89a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"]},{"output_type":"stream","name":"stdout","text":["Hyperparameter tuning started...\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter tuning:   0%|          | 0/6 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Training with parameters: {'learning_rate': 0.001, 'num_epochs': 10}\n","Epoch 1/10, Training Loss: 0.2506\n","Epoch 2/10, Training Loss: 0.1581\n","Epoch 3/10, Training Loss: 0.1229\n","Epoch 4/10, Training Loss: 0.0958\n","Epoch 5/10, Training Loss: 0.0707\n","Epoch 6/10, Training Loss: 0.0556\n","Epoch 7/10, Training Loss: 0.0394\n","Epoch 8/10, Training Loss: 0.0348\n","Epoch 9/10, Training Loss: 0.0255\n","Epoch 10/10, Training Loss: 0.0218\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter tuning:  17%|█▋        | 1/6 [00:30<02:34, 30.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 92.96%\n","Training with parameters: {'learning_rate': 0.001, 'num_epochs': 15}\n","Epoch 1/15, Training Loss: 0.2470\n","Epoch 2/15, Training Loss: 0.1561\n","Epoch 3/15, Training Loss: 0.1179\n","Epoch 4/15, Training Loss: 0.0938\n","Epoch 5/15, Training Loss: 0.0687\n","Epoch 6/15, Training Loss: 0.0501\n","Epoch 7/15, Training Loss: 0.0383\n","Epoch 8/15, Training Loss: 0.0336\n","Epoch 9/15, Training Loss: 0.0252\n","Epoch 10/15, Training Loss: 0.0202\n","Epoch 11/15, Training Loss: 0.0212\n","Epoch 12/15, Training Loss: 0.0171\n","Epoch 13/15, Training Loss: 0.0131\n","Epoch 14/15, Training Loss: 0.0159\n","Epoch 15/15, Training Loss: 0.0144\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter tuning:  33%|███▎      | 2/6 [01:11<02:26, 36.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 92.98%\n","Training with parameters: {'learning_rate': 0.01, 'num_epochs': 10}\n","Epoch 1/10, Training Loss: 0.2461\n","Epoch 2/10, Training Loss: 0.1668\n","Epoch 3/10, Training Loss: 0.1326\n","Epoch 4/10, Training Loss: 0.1095\n","Epoch 5/10, Training Loss: 0.0906\n","Epoch 6/10, Training Loss: 0.0713\n","Epoch 7/10, Training Loss: 0.0657\n","Epoch 8/10, Training Loss: 0.0532\n","Epoch 9/10, Training Loss: 0.0481\n","Epoch 10/10, Training Loss: 0.0429\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter tuning:  50%|█████     | 3/6 [01:38<01:37, 32.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 91.57%\n","Training with parameters: {'learning_rate': 0.01, 'num_epochs': 15}\n","Epoch 1/15, Training Loss: 0.2475\n","Epoch 2/15, Training Loss: 0.1686\n","Epoch 3/15, Training Loss: 0.1343\n","Epoch 4/15, Training Loss: 0.1070\n","Epoch 5/15, Training Loss: 0.0915\n","Epoch 6/15, Training Loss: 0.0734\n","Epoch 7/15, Training Loss: 0.0629\n","Epoch 8/15, Training Loss: 0.0513\n","Epoch 9/15, Training Loss: 0.0488\n","Epoch 10/15, Training Loss: 0.0388\n","Epoch 11/15, Training Loss: 0.0352\n","Epoch 12/15, Training Loss: 0.0339\n","Epoch 13/15, Training Loss: 0.0293\n","Epoch 14/15, Training Loss: 0.0239\n","Epoch 15/15, Training Loss: 0.0248\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter tuning:  67%|██████▋   | 4/6 [02:19<01:11, 35.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 92.11%\n","Training with parameters: {'learning_rate': 0.1, 'num_epochs': 10}\n","Epoch 1/10, Training Loss: 0.9236\n","Epoch 2/10, Training Loss: 0.7256\n","Epoch 3/10, Training Loss: 0.6995\n","Epoch 4/10, Training Loss: 0.7191\n","Epoch 5/10, Training Loss: 0.6959\n","Epoch 6/10, Training Loss: 0.6966\n","Epoch 7/10, Training Loss: 0.6972\n","Epoch 8/10, Training Loss: 0.6964\n","Epoch 9/10, Training Loss: 0.6958\n","Epoch 10/10, Training Loss: 0.6971\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter tuning:  83%|████████▎ | 5/6 [02:48<00:33, 33.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 49.66%\n","Training with parameters: {'learning_rate': 0.1, 'num_epochs': 15}\n","Epoch 1/15, Training Loss: 0.4395\n","Epoch 2/15, Training Loss: 0.4379\n","Epoch 3/15, Training Loss: 0.7113\n","Epoch 4/15, Training Loss: 0.7018\n","Epoch 5/15, Training Loss: 0.7210\n","Epoch 6/15, Training Loss: 0.6976\n","Epoch 7/15, Training Loss: 0.6969\n","Epoch 8/15, Training Loss: 0.6973\n","Epoch 9/15, Training Loss: 0.6962\n","Epoch 10/15, Training Loss: 0.6969\n","Epoch 11/15, Training Loss: 0.6977\n","Epoch 12/15, Training Loss: 0.6966\n","Epoch 13/15, Training Loss: 0.6977\n","Epoch 14/15, Training Loss: 0.6967\n","Epoch 15/15, Training Loss: 0.6972\n"]},{"output_type":"stream","name":"stderr","text":["Hyperparameter tuning: 100%|██████████| 6/6 [03:29<00:00, 34.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 49.66%\n","Best hyperparameters: {'learning_rate': 0.001, 'num_epochs': 15}\n","Best accuracy: 0.9297638184740942\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from transformers import BertModel, BertTokenizer\n","\n","# Load the dataset\n","file_path = '/content/drive/MyDrive/fake_reviews_dataset.csv'\n","df = pd.read_csv(file_path)\n","\n","# Label Encoding\n","le_category = LabelEncoder()\n","df['category'] = le_category.fit_transform(df['category'])\n","\n","le_label = LabelEncoder()\n","df['label'] = le_label.fit_transform(df['label'])\n","\n","# Train-test split\n","X_train_text, X_test_text, y_train, y_test = train_test_split(df['text_'].values, df['label'].values, test_size=0.2, random_state=42)\n","X_train_category, X_test_category = train_test_split(df['category'].values, test_size=0.2, random_state=42)\n","X_train_rating, X_test_rating = train_test_split(df['rating'].values, test_size=0.2, random_state=42)\n","\n","# Tokenization and Dataset Preparation for BERT\n","class FakeReviewBERTDataset(Dataset):\n","    def __init__(self, text, category, rating, labels, max_length):\n","        self.text = text\n","        self.category = category\n","        self.rating = rating\n","        self.labels = labels\n","        self.max_length = max_length\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        # Convert the text to string format\n","        text_str = ' '.join([str(val) for val in self.text[idx]])\n","\n","        inputs = self.tokenizer.encode_plus(\n","            text_str,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': inputs['input_ids'].squeeze(0),\n","            'attention_mask': inputs['attention_mask'].squeeze(0),\n","            'category': torch.tensor(self.category[idx], dtype=torch.float32),\n","            'rating': torch.tensor(self.rating[idx], dtype=torch.float32),\n","            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n","        }\n","\n","max_length = 128  # Adjust as needed\n","train_dataset = FakeReviewBERTDataset(X_train_text, X_train_category, X_train_rating, y_train, max_length)\n","test_dataset = FakeReviewBERTDataset(X_test_text, X_test_category, X_test_rating, y_test, max_length)\n","\n","# DataLoaders\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# BERT Model\n","class FakeReviewBERTModel(nn.Module):\n","    def __init__(self):\n","        super(FakeReviewBERTModel, self).__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.fc = nn.Linear(self.bert.config.hidden_size + 2, 2)\n","\n","    def forward(self, input_ids, attention_mask, x_category, x_rating):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.pooler_output\n","        x = torch.cat([pooled_output, x_category.unsqueeze(1), x_rating.unsqueeze(1)], dim=1)\n","        x = self.fc(x)\n","        return x\n","\n","# Training and Evaluation Functions\n","def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        for i, batch in enumerate(train_loader):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            category = batch['category'].to(device)\n","            rating = batch['rating'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(input_ids, attention_mask, category, rating)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        average_loss = total_loss / len(train_loader)\n","        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_loss:.4f}')\n","\n","def evaluate_model(model, test_loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(test_loader):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            category = batch['category'].to(device)\n","            rating = batch['rating'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(input_ids, attention_mask, category, rating)\n","            _, predicted = torch.max(outputs, 1)\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    if total == 0:\n","        return None\n","    else:\n","        accuracy = correct / total\n","        print(f'Test Accuracy: {accuracy * 100:.2f}%')\n","        return accuracy\n","\n","# Hyperparameters and Initialization\n","num_epochs = 10\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = FakeReviewBERTModel().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=2e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Training and Evaluation\n","train_model(model, train_loader, criterion, optimizer, num_epochs, device)\n","evaluate_model(model, test_loader, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l2zkbci_xLkj","executionInfo":{"status":"ok","timestamp":1713662423114,"user_tz":420,"elapsed":7631538,"user":{"displayName":"Ninad Parthiv Shah","userId":"14593699598119062697"}},"outputId":"25988d5a-ce19-4b90-8ed4-1e2ec9c606f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Training Loss: 0.4169\n","Epoch 2/10, Training Loss: 0.2334\n","Epoch 3/10, Training Loss: 0.1729\n","Epoch 4/10, Training Loss: 0.1309\n","Epoch 5/10, Training Loss: 0.0984\n","Epoch 6/10, Training Loss: 0.0765\n","Epoch 7/10, Training Loss: 0.0617\n","Epoch 8/10, Training Loss: 0.0498\n","Epoch 9/10, Training Loss: 0.0421\n","Epoch 10/10, Training Loss: 0.0352\n","Test Accuracy: 94.83%\n"]},{"output_type":"execute_result","data":{"text/plain":["0.9483121058488932"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/Bert_amazon.pth')"],"metadata":{"id":"oMBLmNTzAeDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = FakeReviewBERTModel().to(device)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/Bert_amazon.pth'))\n","\n","# Continue training using the loaded weights\n","optimizer = optim.Adam(model.parameters(), lr=2e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Additional epochs for further training\n","additional_epochs = 5\n","\n","# Training and Evaluation\n","train_model(model, train_loader, criterion, optimizer, additional_epochs, device)\n","evaluate_model(model, test_loader, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ANcSAgXQ7jQ","executionInfo":{"status":"ok","timestamp":1713666703856,"user_tz":420,"elapsed":3837498,"user":{"displayName":"Ninad Parthiv Shah","userId":"14593699598119062697"}},"outputId":"b28618f3-c980-4c53-c723-31bdd170b09d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5, Training Loss: 0.0330\n","Epoch 2/5, Training Loss: 0.0268\n","Epoch 3/5, Training Loss: 0.0288\n","Epoch 4/5, Training Loss: 0.0208\n","Epoch 5/5, Training Loss: 0.0226\n","Test Accuracy: 94.55%\n"]},{"output_type":"execute_result","data":{"text/plain":["0.9454680351180907"]},"metadata":{},"execution_count":8}]}]}